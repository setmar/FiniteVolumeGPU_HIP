#!/bin/bash -e
#SBATCH --job-name=lumi
#SBATCH --account=project_465001926
#SBATCH --time=00:02:00
#SBATCH --partition=dev-g
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=2
#SBATCH --gpus=2
#SBATCH --gpus-per-node=2
#SBATCH -o %x-%j.out
#SBATCH --exclusive
#

N=$SLURM_JOB_NUM_NODES
echo "--nbr of nodes:", $N
echo "--total nbr of gpus:", $SLURM_NTASKS

Mydir=/project/project_465001926/martinli
Myapplication=${Mydir}/FiniteVolumeGPU_HIP_setmar/mpiTesting.py

#modules
ml LUMI partition/G
ml lumi-container-wrapper
#ml cray-python/3.9.13.1
ml rocm
ml craype-accel-amd-gfx90a
ml cray-mpich

export PATH="/project/project_465001926/martinli/FiniteVolumeGPU_HIP_setmar/MyCondaEnv/bin:$PATH"
export MPICH_GPU_SUPPORT_ENABLED=1

#missing library
#export LD_LIBRARY_PATH=/opt/cray/pe/mpich/8.1.29/ofi/cray/17.0/lib-abi-mpich:$LD_LIBRARY_PATH

#srun --cpu-bind=map_cpu:49,57,17,25,1,9,33,41 bash -c ' \
#     echo -n "task $SLURM_PROCID (node $SLURM_NODEID): "; \
#     taskset -cp $$' | sort

srun --cpu-bind=map_cpu:49,57,17,25,1,9,33,41 --mpi=pmi2 python ${Myapplication} -nx 1024 -ny 1024 --profile

